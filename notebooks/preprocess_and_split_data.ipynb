{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a8cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectorMixin, VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e829fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"../data/full_data_set.csv\")\n",
    "data_set = data_set.drop(columns=['class'])\n",
    "target_col = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750a624",
   "metadata": {},
   "source": [
    "### 1. Drop highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a28a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationThreshold(SelectorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Feature selector that removes correlated features.\n",
    "\n",
    "    This feature selection algorithm looks only at the features (X), not the\n",
    "    desired outputs (y), and can thus be used for unsupervised learning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float\n",
    "        Pairwise correlation threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold: float = None) -> None:\n",
    "        self.threshold = threshold if threshold is not None else 1.0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn empirical correlations from X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Data from which to compute correlations, where `n_samples` is the\n",
    "            number of samples and `n_features` is the number of features.\n",
    "        y : any, default=None\n",
    "            Ignored. This parameter exists only for compatibility with\n",
    "            sklearn.pipeline.Pipeline.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        corr = np.abs(np.corrcoef(X, rowvar=False))\n",
    "        self.mask = ~(np.tril(corr, k=-1) > self.threshold).any(axis=1)\n",
    "        return self\n",
    "\n",
    "    def _get_support_mask(self):\n",
    "        return self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8590cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "scaler = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            VarianceThreshold(),\n",
    "            CorrelationThreshold(0.90),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5985383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline\n",
    "preprocessing_pipeline = scaler.fit(data_set.drop(columns=['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a055094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "preprocessed_data_set = preprocessing_pipeline.transform(data_set.drop(columns=['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a264c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nntc', 'Tot2DU', 'TotMaxDU', 'MaxTotDF', 'TotMaxDF']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieved highly correlated columns\n",
    "input_columns = data_set.drop(columns=['label']).columns\n",
    "remaining_columns = input_columns  \n",
    "for step_name, step in preprocessing_pipeline.named_steps.items():\n",
    "    if hasattr(step, 'get_support'):  \n",
    "        mask = step.get_support()\n",
    "        remaining_columns = remaining_columns[mask]\n",
    "\n",
    "list(set(input_columns) - set(remaining_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ac4e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBO', 'WMC', 'DIT', 'NOC', 'RFC', 'LCOM', 'NOM', 'NOPM', 'NOSM', 'NOF',\n",
       "       'NOPF', 'NOSF', 'NOSI', 'LOC', 'Tot2Op', 'Max2Op', 'TotMaxOp',\n",
       "       'MaxTotOp', 'Tot2Lev', 'Max2Lev', 'TotMaxLev', 'MaxTotLev', 'Tot2DF',\n",
       "       'Max2DF', 'Max2DU', 'MaxTotDU', 'TotInMetCall', 'MaxInMetCall',\n",
       "       'InOutDeg', 'PubMembers', 'Ncf', 'Nuf', 'Ncs', 'Ns', 'Nf', 'Ntsc',\n",
       "       'Ndsc', 'Runtime', 'PassTestRatio', 'FailTestRatio', 'TotPassTestRatio',\n",
       "       'TotFailTestRatio', 'NTestRunPerRT', 'Um', 'Md', 'Nmd', 'Gs', 'DDU'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195dcc8",
   "metadata": {},
   "source": [
    "### 2. Create a balanced set via undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbafe019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "labels = data_set['label']\n",
    "labels = pd.Series(labels)\n",
    "label_to_int = {\n",
    "    'Non-Faulty': 0,\n",
    "    'Weakly-Faulty': 1,\n",
    "    'Fairly-Faulty': 2,\n",
    "    'Faulty': 3,\n",
    "    'Strongly-Faulty': 4\n",
    "}\n",
    "class_labels = labels.map(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537b8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling data to create balanced set to the class (Strongly-Faulty) with the minimum number of data points\n",
    "unique_classes = np.unique(class_labels)\n",
    "X_resampled_list = []\n",
    "y_resampled_list = []\n",
    "# Find size of the smallest class\n",
    "min_class_size = min([np.sum(class_labels == label) for label in unique_classes])\n",
    "# Undersample each class\n",
    "for label in unique_classes:\n",
    "    X_class = preprocessed_data_set[class_labels == label]\n",
    "    y_class = class_labels[class_labels == label]\n",
    "    \n",
    "    X_class_undersampled, y_class_undersampled = resample(\n",
    "        X_class, y_class, n_samples=min_class_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_resampled_list.append(X_class_undersampled)\n",
    "    y_resampled_list.append(y_class_undersampled)\n",
    "\n",
    "# Concatenate all the resampled data\n",
    "X_balanced = np.vstack(X_resampled_list)\n",
    "y_balanced = np.hstack(y_resampled_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b5989",
   "metadata": {},
   "source": [
    "### 3. split into development / test sets (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa3cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_set, X_test, y_dev_set, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, stratify=y_balanced, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "590714a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1640, 48)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a9c353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410, 48)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the development set\n",
    "with open('development_set.pkl', 'wb') as f: # Same set can be found in ../data/development_set.pkl\n",
    "    pickle.dump((X_dev_set, y_dev_set), f)\n",
    "# Save the test set\n",
    "with open('test_set.pkl', 'wb') as f: # Same set can be found in ../data/test_set.pkl\n",
    "    pickle.dump((X_test, y_test), f)\n",
    "# Save the list of remaining_columns to a pickle file\n",
    "with open('metrics_concidered.pkl', 'wb') as file: # Same set can be found in ../data/metrics_concidered.pkl\n",
    "    pickle.dump(remaining_columns.tolist(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2d165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
